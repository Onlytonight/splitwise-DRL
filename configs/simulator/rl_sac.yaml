# SAC强化学习算法配置
algorithm: sac

# 决策参数
decision_interval: 2  # 决策间隔（秒）

# 状态特征配置
enabled_features:
#  - queue              # 队列特征 (p_queue, d_queue)
#  - none_count         # None计数 (prompt_none_count, token_none_count)
  - instance_count     # 实例数 (n_p, n_t) - 归一化资源状态
  - timestamp          # 时间戳
  - rps                # 请求到达率 - norm_arrival_token_rate
  - rps_delta          # RPS变化率 - traffic_trend
#  - length             # 长度特征 (prompt_len, output_len)
#  - rate               # 速率特征 (prompt_rate, token_rate)
  - util_mem           # 内存利用率 - d_memory_util_level
#  - draining           # 缩容状态
  - p_ins_pending_token  # P实例pending - p_saturation_level
#  - queue_delta        # 队列变化率
  - io_ratio           # 输入/输出长度比
  - avg_batch_tokens   # 平均批次token数
  - rejection_rate     # 拒绝率 (p_rejection_rate, d_rejection_rate)

stack_size: 1  # 状态堆叠的时间窗大小
debug_features: false  # 是否启用调试特征

# 奖励函数权重
reward_weights:
  w_cost: 0.2      # 成本惩罚权重
  w_queue: 10.0     # 队列惩罚权重（目标：队列数为0）
  w_util: 10.0      # 利用率权重
  w_congestion: 10.0 # 拥挤惩罚权重
  # 保留旧参数以兼容（已废弃）
  w_stability: 0.0
  w_slo: 0.0
  w_switch: 0.0

# 动作执行参数
action_scale_step: 20
action_mig_step: 20
min_instances_per_pool: 1
max_total_instances: 200

# SAC网络超参数
network:
  layer_size: 256  # 网络隐藏层大小

# SAC训练超参数
training:
  replay_buffer_size: 1000000  # 经验回放缓冲区大小
  batch_size: 256              # 批量大小
  train_freq: 30               # 训练频率（每多少步训练一次）
  min_steps_before_training: 2000  # 开始训练前的最小步数
  discount: 0.99               # 折扣因子
  soft_target_tau: 0.005       # 软目标更新系数
  target_update_period: 1      # 目标网络更新周期
  policy_lr: 0.0003            # 策略网络学习率
  qf_lr: 0.0003                # Q网络学习率
  reward_scale: 1              # 奖励缩放
  use_automatic_entropy_tuning: true  # 是否使用自动熵调节

# 优先经验回放参数
prioritized_replay:
  alpha: 0.6                   # 优先级指数
  beta: 0.4                    # 重要性采样初始值
  beta_increment_per_sampling: 0.001  # beta增量
  eps: 0.000001                # 防止除零的小值

# 模型保存
save_model_freq: 1000          # 保存模型频率（每多少步）
checkpoint_dir: cp             # checkpoint保存目录

# 仿真评估模式（如果指定了model_path则为true）
# eval_only: false  # 是否仅评估模式（不训练）

# 经验池加载配置（用于从baseline策略预训练）
# load_replay_buffer_path: null  # 经验池文件路径（.npz格式），如果指定则在初始化时加载
load_replay_buffer_path: "data/replay_buffer_heteroscale.npz"
