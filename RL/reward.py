import numpy as np
import logging
import csv
import os
from collections import defaultdict
import pandas as pd

class RLRewardCalculator:
    def __init__(self,
                 config,
                 max_instances=100,
                 price_ratio_token=0.6,
                 mode: str = "joint"):
        """
        :param config: åŒ…å«æƒé‡å‚æ•°çš„é…ç½®å¯¹è±¡ (DictConfig)
        :param max_instances: é›†ç¾¤æœ€å¤§æœºå™¨æ•° (ç”¨äºå½’ä¸€åŒ–æˆæœ¬)
        :param price_ratio_token: Token æœºå™¨ç›¸å¯¹äº Prompt æœºå™¨çš„æˆæœ¬æ¯”ç‡
                                  (ä¾‹å¦‚ H100=1.0, A100=0.6)
        :param mode: "prompt" / "token" / "joint"ï¼Œç”¨äºåŒºåˆ†å¥–åŠ±é€»è¾‘
        """
        assert mode in ("prompt", "token", "joint")
        self.mode = mode
        # 1. æƒé‡å‚æ•° (éœ€è¦é€šè¿‡è¶…å‚æ•°æœç´¢å¾®è°ƒ)
        # å»ºè®®åˆå§‹å€¼: w_cost=0.5, w_slo=2.0, w_switch=0.1, w_util=0.2
        self.w_cost = config.get("w_cost", 0.5)
        self.w_slo = config.get("w_slo", 2.0)
        self.w_switch = config.get("w_switch", 0.1)
        self.w_util = config.get("w_util", 0.2)

        # 2. ç¡¬ä»¶æˆæœ¬å‚æ•°
        self.price_p = 1.0  # Prefill æœºå™¨ (åŸºå‡†ä»·æ ¼)
        # self.price_t = price_ratio_token  # Decoding æœºå™¨ (é€šå¸¸è¾ƒä¾¿å®œ)
        self.price_t = 1.0  # å‡è®¾éƒ½æ˜¯ç”¨åŒæ ·çš„æœºå™¨
        self.max_instances = max_instances
        self.is_first_step = True

        # 3. çŠ¶æ€è®°å¿† (ç”¨äºè®¡ç®—åˆ‡æ¢æˆæœ¬)
        self.last_instances = {'p': 0, 't': 0}
        self.last_action_sign = 0 # è®°å½•ä¸Šä¸€æ¬¡æ˜¯åŠ è¿˜æ˜¯å‡

    #     new
        self.BASE_SLO_PENALTY = 10.0
        self.ACTION_COST = 0.2
        self.HYSTERESIS_PENALTY = 2.0
        self.max_instances = max_instances
        self.last_action_sign = 0

        # SLO é˜ˆå€¼ (å•ä½: ç§’)
        self.TARGET_TTFT = 1.0  # 1ç§’
        self.TARGET_TBT = 0.05  # 50ms

    def calculate_reward(self, cluster, applications, raw_stats, instance_num, action_executed=True, step=0):
        """
        åŸºäºæ’é˜Ÿè®ºä¼°ç®—çš„å³æ—¶å¥–åŠ±ï¼Œä¿®å¤é©¬å°”å¯å¤«æ€§ç ´åé—®é¢˜ã€‚
        
        :param avg_queue_time: å¹³å‡é˜Ÿåˆ—æ—¶é—´
        :param avg_nth_token_overhead: å¹³å‡ nth_token_overhead
        """

        # -------------------------------------------------------------
        # 1. è·å–å³æ—¶çŠ¶æ€ (Leading Indicators)
        # -------------------------------------------------------------

        # A. é˜Ÿåˆ—å †ç§¯æƒ…å†µ
        # è¿™æ˜¯â€œæ­£åœ¨å‘ç”Ÿçš„ç¾éš¾â€,æ€»promptæ•°
        q_prompt = raw_stats[2]
        q_decoding = raw_stats[3]

        # B. å½“å‰ç³»ç»Ÿçš„å¤„ç†èƒ½åŠ› (Service Rate)
        # æˆ‘ä»¬éœ€è¦çŸ¥é“å½“å‰ 1ç§’ èƒ½æ¶ˆåŒ–å¤šå°‘è¯·æ±‚ã€‚
        # å¯ä»¥ç”¨è¿‡å»ä¸€ä¸ªå°çª—å£çš„å¹³å‡ååé‡æ¥è¿‘ä¼¼å½“å‰çš„å¤„ç†èƒ½åŠ›ã€‚
        # raw_stats éœ€è¦åŒ…å« 'processed_prompt_reqs_per_sec' å’Œ 'processed_token_reqs_per_sec'
        # max(0.1, ...) é˜²æ­¢é™¤ä»¥é›¶
        throughput_p = max(0.1, raw_stats[0])
        throughput_d = max(0.1, raw_stats[1])

        # -------------------------------------------------------------
        # 2. è®¡ç®—â€œå³æ—¶ä¼°ç®—å»¶è¿Ÿâ€ (Instantaneous Estimated Latency)
        # -------------------------------------------------------------

        # ä¼°ç®— TTFTï¼šå¦‚æœåœ¨ Prefill é˜Ÿåˆ—æ’é˜Ÿï¼Œè¦æ’å¤šä¹…ï¼Ÿ
        # å…¬å¼ï¼šæ’é˜Ÿæ•° / æ¶ˆåŒ–é€Ÿåº¦
        est_ttft = q_prompt / throughput_p

        # ä¼°ç®— TBT å‹åŠ›ï¼šè¿™é‡Œæ¯”è¾ƒç‰¹æ®Šã€‚
        # TBT å˜å·®é€šå¸¸æ˜¯å› ä¸º Decoding æœºå™¨æ˜¾å­˜æ»¡äº†ï¼Œè¯·æ±‚è¿›ä¸å» Decoding Poolï¼Œ
        # æˆ–è€… Decoding Pool å¹¶å‘è¿‡é«˜å¯¼è‡´æ˜¾å­˜å¸¦å®½äº‰æŠ¢ã€‚
        # æˆ‘ä»¬å¯ä»¥ç”¨ (Decodingé˜Ÿåˆ— / Decodingæ¶ˆåŒ–é€Ÿåº¦) æ¥è¡¡é‡â€œç­‰å¾…è¿›å…¥ Decoding çš„å»¶è¿Ÿâ€ã€‚
        # å¦‚æœ Decoding é˜Ÿåˆ—åœ¨å †ç§¯ï¼Œè¯´æ˜ TBT é£é™©æå¤§ (å› ä¸ºå‰é¢çš„è¯·æ±‚å¡ä½äº†)ã€‚
        est_decoding_wait = q_decoding / throughput_d

        request_data = []
        request_data.append({
            'prompt_sizes': raw_stats[6],
            'ttft': est_ttft,
            'tbt': est_decoding_wait
        })
        request_df = pd.DataFrame(request_data)
        normalized_df = applications[0].scheduler.perf_model.add_baseline_perf(request_df, model="bloom-176b", hardware="h100-80gb",
                                                          tensor_parallel=8)


        # å½’ä¸€åŒ–ä¸º Ratio (ç›¸å¯¹äº SLO é˜ˆå€¼)
        # æ³¨æ„ï¼šprompt agent ä¸»è¦çœ‹ TTFTï¼Œtoken agent ä¸»è¦çœ‹ TBT
        # ratio_ttft = est_ttft / self.TARGET_TTFT
        # ratio_tbt = est_decoding_wait / (self.TARGET_TBT * 10)  # å®¹å¿åº¦ç¨å¾®æ”¾å®½
        normalized_df['normalized_ttft'] = normalized_df['ttft'] / normalized_df['baseline_ttft']
        normalized_df['normalized_tbt'] = normalized_df['tbt'] / normalized_df['baseline_tbt']
        ratio_ttft = normalized_df['normalized_ttft'][0] / 6
        ratio_tbt = normalized_df['normalized_tbt'][0] / 5

        if self.mode == "prompt":
            max_ratio = ratio_ttft
        elif self.mode == "token":
            max_ratio = ratio_tbt
        else:
            max_ratio = max(ratio_ttft, ratio_tbt)

        # -------------------------------------------------------------
        # 3. è®¡ç®—å¥–åŠ± (é€»è¾‘ä¸ä¹‹å‰ç›¸åŒï¼Œä½†è¾“å…¥å˜äº†)
        # -------------------------------------------------------------

        # è®¡ç®—æˆæœ¬åˆ†æ•°
        n_p, n_t = raw_stats[4:6]
        if self.mode == "prompt":
            cost_score = n_p
        elif self.mode == "token":
            cost_score = n_t
        else:
            cost_score = (n_p + n_t)
        # æœ€å¤§å®ä¾‹æ•°*interval step
        # è®¡ç®— prompt å®ä¾‹çš„æ€»ä½¿ç”¨æ—¶é—´ï¼ˆè‡ªä¸Šæ¬¡è°ƒç”¨ä»¥æ¥ï¼‰
        # if self.mode == "prompt":
        #     cost_score = applications[0].scaling_manager.calculate_prompt_instance_time_since_last()
        # elif self.mode == "token":
        #     cost_score = applications[0].scaling_manager.calculate_token_instance_time_since_last()

        reward = 0.0
        # å¥–åŠ±1.0
        # if max_ratio > 1.0:
        #     # === ğŸ”´ å±é™©åŒº ===
        #     # é˜Ÿåˆ—å †ç§¯å¯¼è‡´é¢„ä¼°å»¶è¿Ÿè¶…æ ‡ï¼Œç«‹åˆ»é‡ç½šï¼
        #     # è¿™æ · Agent åœ¨é˜Ÿåˆ—åˆšå¼€å§‹å †ç§¯ï¼ˆtæ—¶åˆ»ï¼‰å°±ä¼šæ”¶åˆ°è´Ÿåé¦ˆï¼Œä¸ç”¨ç­‰è¯·æ±‚è·‘å®Œã€‚
        #     reward = -self.BASE_SLO_PENALTY * ((max_ratio - 1.0) ** 2) - 2.0
        #
        # elif max_ratio > 0.8:
        #     # === ğŸŸ¡ ç¼“å†²åŒº ===
        #     reward = (1.0 - cost_score) + 0.5
        #
        # else:
        #     # === ğŸŸ¢ å®‰å…¨åŒº ===
        #     reward = 1.0 - cost_score

        # å¥–åŠ±2.0
        # reward_stats = [prompt_rate, token_rate, p_queue, d_queue, n_p, n_t, avg_prompt_size, ttft_rate, tbt_rate, p_queue_len, d_queue_len]
        # Agent ä¼šä¸ºäº†è®© penalty_slo ä¿æŒä¸º 0 è€Œä¸æ•¢è¶Šçº¿
        # åœ¨ penalty_slo ä¸º 0 çš„å‰æä¸‹ï¼Œå®ƒä¼šå°½å¯èƒ½å‡å° cost_term
        queue_len = raw_stats[2] if self.mode == "prompt" else raw_stats[3]
        if self.mode == "prompt":
            use_time = applications[0].scaling_manager.calculate_prompt_instance_time_since_last()
        else :
            use_time = applications[0].scaling_manager.calculate_token_instance_time_since_last()
        reward = -3 * (queue_len/10000)
        # - self.w_cost * use_time
        # print(-self.w_slo * np.log1p(q_prompt),- self.w_cost * cost_score)

        # å¥–åŠ±3.0
        #   ä¸å¯¹ï¼Œç°åœ¨çš„æ’é˜Ÿåº”è¯¥æ˜¯è¶Šå°‘è¶Šå¥½ï¼Œé€‚å½“çš„
        # 1. æ‚¬å´–ï¼šè¿çº¦ (Ratio > 1.0)
        #     reward =  -self.w_slo * (max_ratio ** 2)  # é‡ç½š
        # # 2. ç”œå¤´ï¼šé»„é‡‘åŒºé—´ (0.8 < Ratio <= 1.0)
        # # è¿™å°±æ˜¯è®© Agent å‹‡æ•¢çš„åŸå› ï¼åªè¦ä¿æŒåœ¨è¿™é‡Œï¼Œä¸ä»…ä¸ç½šï¼Œè¿˜ç»™é¢å¤–çš„æ­£åé¦ˆ
        # elif max_ratio > 0.8:
        #     reward = 2 - cost_score + max_ratio * 10
        # # 3. æµªè´¹ï¼šç¦»æ‚¬å´–å¤ªè¿œ (Ratio <= 0.8) å³ä½¿æ²¡è¿çº¦ï¼Œä½†å› ä¸ºç¦»å¾—å¤ªè¿œï¼Œæ²¡æœ‰â€œç”œå¤´â€æ‹¿ï¼Œåªæœ‰çœé’±çš„å¾®è–„å¥–åŠ±
        # else:
        #     reward = - cost_score * ((1-max_ratio)*100)**2

        # å¥–åŠ±4.0

        # -------------------------------------------------------------
        # 4. ç¨³å®šæ€§æƒ©ç½š
        # -------------------------------------------------------------
        # (ä¿æŒåŸæœ‰çš„è¿Ÿæ»æƒ©ç½šé€»è¾‘)
        # ...
        info = {
            'step':step,
            'reward':reward,
            'cost_score': cost_score,
            'ttft_p50':raw_stats[7][0],
            'ttft_p90':raw_stats[7][1],
            'ttft_p99':raw_stats[7][2],
            'tbt_p50':raw_stats[8][0],
            'tbt_p90':raw_stats[8][1],
            'tbt_p99':raw_stats[8][2],
            'p_queue_len':raw_stats[2], #sch_queue
            't_queue_len':raw_stats[3],
            'instance_p_queue_len':raw_stats[9],
            'instance_t_queue_len':raw_stats[10],
            'use_time':use_time,
            'avg_queue_time': raw_stats[11],
            'avg_nth_token_overhead': raw_stats[12]
        }
        return reward,info

    def calculate_reward_(self, cluster, applications, interval_stats, instance_num, action_executed=True):
        """
        è®¡ç®—å•æ­¥å¥–åŠ±
        :param cluster: Cluster å¯¹è±¡
        :param applications: App å¯¹è±¡
        :param interval_stats: åŒ…å« TTFTå’ŒTBTçš„P50ã€P90ã€P99å€¼çš„åˆ—è¡¨
                               æ ¼å¼ï¼š[[ttft_p50, ttft_p90, ttft_p99], [tbt_p50, tbt_p90, tbt_p99], [ttft_vio, tbt_vio]]
        :param instance_num: å®ä¾‹æ•°é‡å’Œåˆ©ç”¨ç‡
        :param action_executed: æ˜¯å¦æ‰§è¡Œäº†æ‰©ç¼©å®¹åŠ¨ä½œ
        :return: (total_reward, info_dict)
        """

        # --- A. è¿è¥æˆæœ¬é¡¹ (OpEx) ---
        # ç›®æ ‡ï¼šæœ€å°åŒ–ç§Ÿé‡‘
        n_p, n_t = instance_num[0], instance_num[1]

        # è®¡ç®—åŠ æƒæˆæœ¬ (Normalized by max budget)
        current_cost = (n_p * self.price_p + n_t * self.price_t)
        max_possible_cost = self.max_instances * 1.0

        cost_penalty = -current_cost

        # --- B. SLO å¥–åŠ±é¡¹ (Performance) ---
        # ä½¿ç”¨ P50ã€P90ã€P99 çš„ TTFT å’Œ TBT è®¡ç®— SLO åˆè§„æ€§
        # SLO é˜ˆå€¼ï¼š
        # TTFT: P50=2, P90=3, P99=6
        # TBT: P50=1.25, P90=1.5, P99=5
        
        ttft_values = interval_stats[0]  # [p50, p90, p99]
        tbt_values = interval_stats[1]   # [p50, p90, p99]
        
        # TTFT SLO é˜ˆå€¼
        ttft_slo_thresholds = [2.0, 3.0, 6.0]  # P50, P90, P99
        # TBT SLO é˜ˆå€¼
        tbt_slo_thresholds = [1.25, 1.5, 5.0]  # P50, P90, P99
        
        # è®¡ç®—æ¯ä¸ªåˆ†ä½æ•°çš„åˆè§„ç‡ï¼ˆå€¼è¶Šå°äºé˜ˆå€¼è¶Šå¥½ï¼‰
        ttft_compliance_scores = []
        tbt_compliance_scores = []
        
        # å¥–åŠ±é€»è¾‘ï¼šSLO ç¬¦åˆ (â‰¤ é˜ˆå€¼) ç»™å°å¥–åŠ±ï¼Œä¸ç¬¦åˆ (> é˜ˆå€¼) ç»™æƒ©ç½š
        ttft_compliance_scores = []
        tbt_compliance_scores = []
        for i in range(3):
            # TTFT
            if ttft_values[i] <= ttft_slo_thresholds[i]:
                ttft_score = 10  # ç¬¦åˆ SLO ç»™å°å¥–åŠ±
            else:
                ttft_score = -1 * (ttft_values[i] - ttft_slo_thresholds[i]) *10 # è¶…å‡º SLO ç»™æƒ©ç½šï¼ŒæŒ‰è¶…å‡ºæ¯”ä¾‹çº¿æ€§
            ttft_compliance_scores.append(ttft_score)
            # TBT
            if tbt_values[i] <= tbt_slo_thresholds[i]:
                tbt_score = 10
            else:
                tbt_score = -1 * (tbt_values[i] - tbt_slo_thresholds[i]) *10
            tbt_compliance_scores.append(tbt_score)
        # åŠ æƒå¹³å‡
        weights = [0.2, 0.3, 0.5]
        ttft_weighted_score = sum(w * s for w, s in zip(weights, ttft_compliance_scores))
        tbt_weighted_score = sum(w * s for w, s in zip(weights, tbt_compliance_scores))
        
        # ç»¼åˆ SLO å¥–åŠ±ï¼ˆTTFT å’Œ TBT å„å ä¸€åŠï¼‰
        slo_reward = 0.5 * ttft_weighted_score + 0.5 * tbt_weighted_score
        

        # --- C. åˆ‡æ¢æˆæœ¬é¡¹ & ç¨³å®šæ€§å¥–åŠ± (Stability) ---
        # ç›®æ ‡ï¼šæŠ‘åˆ¶æœºå™¨æ•°é‡å‰§çƒˆæŠ–åŠ¨ï¼Œå¥–åŠ±ç¨³å®šçŠ¶æ€
        stability_bonus = 0.0
        
        if self.is_first_step:
            self.is_first_step = False
            switch_penalty = 0.0
        else:
            # å¦‚æœæ²¡æœ‰æ‰§è¡Œæ‰©ç¼©å®¹åŠ¨ä½œï¼ˆdelta_total == 0 æˆ– action_executed == Falseï¼‰
            if not action_executed:
                # ç»™äºˆç¨³å®šæ€§å¥–åŠ±
                stability_bonus = 5  # é¼“åŠ±ä¿æŒç¨³å®š
                switch_penalty = 0.0
            else:
                switch_penalty = -5

    
        delta_p = abs(n_p - self.last_instances['p'])
        delta_t = abs(n_t - self.last_instances['t'])
        delta_total = delta_p + delta_t
            
        # æ›´æ–°å†å²
        self.last_instances = {'p': n_p, 't': n_t}

        # --- D. åˆ©ç”¨ç‡å¡‘å½¢ (Reward Shaping - Optional) ---
        # ç›®æ ‡ï¼šå¼•å¯¼ Agent å°†åˆ©ç”¨ç‡ç»´æŒåœ¨ "Sweet Spot" (ä¾‹å¦‚ 60% - 80%)
        # é¿å… 0% (æµªè´¹) ä¹Ÿä¸è¦ 100% (å®¹æ˜“æ’é˜Ÿ)
        util_p,util_d = instance_num[2],instance_num[3]

        def utilization_bonus(u):
            # ä¸€ä¸ªå€’ U å‹å‡½æ•°ï¼Œåœ¨ 0.7 å¤„è¾¾åˆ°å³°å€¼ 1.0
            # æ”¹ä¸ºæŒ‡æ•°å½¢å¼ä»¥å¢å¼ºæ•æ„Ÿåº¦
            return np.exp(1.0 - abs(u - 0.7)) / np.e

        util_reward = 0.3 * utilization_bonus(util_p) + 0.3 * utilization_bonus(util_d)

        # --- E. æ€»å¥–åŠ±èšåˆ ---
        # æ³¨æ„ï¼šCost å’Œ Switch æ˜¯è´Ÿå€¼ï¼ŒSLOã€Util å’Œ Stability æ˜¯æ­£å€¼
        # æ ‡å‡†åŒ–å„ç»„ä»¶å€¼åˆ°ç›¸ä¼¼èŒƒå›´ï¼Œä¿æŒç¬¦å·ä¸å˜
        total_reward = (
                self.w_cost * cost_penalty +
                self.w_slo * slo_reward +
                self.w_switch * (switch_penalty + stability_bonus)
                # self.w_util * util_reward
        )

        # è¿”å›è¯¦ç»†ä¿¡æ¯ç”¨äº Debug (è¿™å¯¹ RL è°ƒå‚è‡³å…³é‡è¦ï¼)
        info = {
            "reward_total": total_reward,
            "raw_cost": current_cost,
            "pen_cost": self.w_cost * cost_penalty,
            "rew_slo": self.w_slo * slo_reward,
            "pen_switch": self.w_switch * switch_penalty,
            "rew_stability": self.w_switch * stability_bonus,
            "rew_util": self.w_util * util_reward,
            "ttft_weighted": ttft_weighted_score,
            "tbt_weighted": tbt_weighted_score,
            "ttft_p50": ttft_values[0] if len(ttft_values) > 0 else 0,
            "ttft_p90": ttft_values[1] if len(ttft_values) > 1 else 0,
            "ttft_p99": ttft_values[2] if len(ttft_values) > 2 else 0,
            "tbt_p50": tbt_values[0] if len(tbt_values) > 0 else 0,
            "tbt_p90": tbt_values[1] if len(tbt_values) > 1 else 0,
            "tbt_p99": tbt_values[2] if len(tbt_values) > 2 else 0,
            "delta_total": delta_total,
            "action_executed": action_executed,
            "util_avg": (util_p + util_d) / 2
        }

        return total_reward, info

    def reset(self):
        """é‡ç½®å†…éƒ¨çŠ¶æ€ (æ¯ä¸ª Episode å¼€å§‹æ—¶è°ƒç”¨)"""
        self.last_instances = {'p': 0, 't': 0}

class RewardRecorder:


    def __init__(self, filename="reward.csv", clear_file=True):
        self.filename = filename
        self.fieldnames = None  # å°†åœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨record_rewardæ—¶åˆå§‹åŒ–
        self._initialize_csv(clear_file)

    def _initialize_csv(self, clear_file=True):
        """Initialize the CSV file with headers"""
        # å¦‚æœéœ€è¦æ¸…ç©ºæ–‡ä»¶æˆ–è€…æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™é‡æ–°åˆ›å»ºæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´
        if clear_file or not os.path.exists(self.filename):
            with open(self.filename, 'w', newline='') as csvfile:
                pass  # åªåˆ›å»ºç©ºæ–‡ä»¶

    def record_reward(self, step, info_dict):
        """
        Record reward components to CSV file

        :param step: Current decision step
        :param info_dict: Dictionary containing reward components from RLRewardCalculator
        """

        # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼Œåˆå§‹åŒ–fieldnameså¹¶å†™å…¥è¡¨å¤´
        if self.fieldnames is None:
            self.fieldnames = list(info_dict.keys())
            with open(self.filename, 'w', newline='') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=self.fieldnames)
                writer.writeheader()

        # ä½¿ç”¨è¿½åŠ æ¨¡å¼å†™å…¥æ•°æ®
        with open(self.filename, 'a', newline='') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=self.fieldnames)
            writer.writerow(info_dict)