import numpy as np
import logging
import csv
import os
from collections import defaultdict

class RLRewardCalculator:
    def __init__(self,
                 config,
                 max_instances=100,
                 price_ratio_token=0.6):
        """
        :param config: åŒ…å«æƒé‡å‚æ•°çš„é…ç½®å¯¹è±¡ (DictConfig)
        :param max_instances: é›†ç¾¤æœ€å¤§æœºå™¨æ•° (ç”¨äºå½’ä¸€åŒ–æˆæœ¬)
        :param price_ratio_token: Token æœºå™¨ç›¸å¯¹äº Prompt æœºå™¨çš„æˆæœ¬æ¯”ç‡
                                  (ä¾‹å¦‚ H100=1.0, A100=0.6)
        """
        # 1. æƒé‡å‚æ•° (éœ€è¦é€šè¿‡è¶…å‚æ•°æœç´¢å¾®è°ƒ)
        # å»ºè®®åˆå§‹å€¼: w_cost=0.5, w_slo=2.0, w_switch=0.1, w_util=0.2
        self.w_cost = config.get("w_cost", 0.5)
        self.w_slo = config.get("w_slo", 2.0)
        self.w_switch = config.get("w_switch", 0.1)
        self.w_util = config.get("w_util", 0.2)

        # 2. ç¡¬ä»¶æˆæœ¬å‚æ•°
        self.price_p = 1.0  # Prefill æœºå™¨ (åŸºå‡†ä»·æ ¼)
        # self.price_t = price_ratio_token  # Decoding æœºå™¨ (é€šå¸¸è¾ƒä¾¿å®œ)
        self.price_t = 1.0  # å‡è®¾éƒ½æ˜¯ç”¨åŒæ ·çš„æœºå™¨
        self.max_instances = max_instances
        self.is_first_step = True

        # 3. çŠ¶æ€è®°å¿† (ç”¨äºè®¡ç®—åˆ‡æ¢æˆæœ¬)
        self.last_instances = {'p': 0, 't': 0}
        self.last_action_sign = 0 # è®°å½•ä¸Šä¸€æ¬¡æ˜¯åŠ è¿˜æ˜¯å‡

    #     new
        self.BASE_SLO_PENALTY = 10.0
        self.ACTION_COST = 0.2
        self.HYSTERESIS_PENALTY = 2.0
        self.max_instances = max_instances
        self.last_action_sign = 0

        # SLO é˜ˆå€¼ (å•ä½: ç§’)
        self.TARGET_TTFT = 1.0  # 1ç§’
        self.TARGET_TBT = 0.05  # 50ms

    def calculate_reward(self, cluster, applications, raw_stats, instance_num, action_executed=True, step=0):
        """
        åŸºäºæ’é˜Ÿè®ºä¼°ç®—çš„å³æ—¶å¥–åŠ±ï¼Œä¿®å¤é©¬å°”å¯å¤«æ€§ç ´åé—®é¢˜ã€‚
        """

        # -------------------------------------------------------------
        # 1. è·å–å³æ—¶çŠ¶æ€ (Leading Indicators)
        # -------------------------------------------------------------

        # A. é˜Ÿåˆ—å †ç§¯æƒ…å†µ
        # è¿™æ˜¯â€œæ­£åœ¨å‘ç”Ÿçš„ç¾éš¾â€
        q_prompt = raw_stats[2]
        q_decoding = raw_stats[3]

        # B. å½“å‰ç³»ç»Ÿçš„å¤„ç†èƒ½åŠ› (Service Rate)
        # æˆ‘ä»¬éœ€è¦çŸ¥é“å½“å‰ 1ç§’ èƒ½æ¶ˆåŒ–å¤šå°‘è¯·æ±‚ã€‚
        # å¯ä»¥ç”¨è¿‡å»ä¸€ä¸ªå°çª—å£çš„å¹³å‡ååé‡æ¥è¿‘ä¼¼å½“å‰çš„å¤„ç†èƒ½åŠ›ã€‚
        # raw_stats éœ€è¦åŒ…å« 'processed_prompt_reqs_per_sec' å’Œ 'processed_token_reqs_per_sec'
        # max(0.1, ...) é˜²æ­¢é™¤ä»¥é›¶
        throughput_p = max(0.1, raw_stats[0])
        throughput_d = max(0.1, raw_stats[1])

        # -------------------------------------------------------------
        # 2. è®¡ç®—â€œå³æ—¶ä¼°ç®—å»¶è¿Ÿâ€ (Instantaneous Estimated Latency)
        # -------------------------------------------------------------

        # ä¼°ç®— TTFTï¼šå¦‚æœåœ¨ Prefill é˜Ÿåˆ—æ’é˜Ÿï¼Œè¦æ’å¤šä¹…ï¼Ÿ
        # å…¬å¼ï¼šæ’é˜Ÿæ•° / æ¶ˆåŒ–é€Ÿåº¦
        est_ttft = q_prompt / throughput_p

        # ä¼°ç®— TBT å‹åŠ›ï¼šè¿™é‡Œæ¯”è¾ƒç‰¹æ®Šã€‚
        # TBT å˜å·®é€šå¸¸æ˜¯å› ä¸º Decoding æœºå™¨æ˜¾å­˜æ»¡äº†ï¼Œè¯·æ±‚è¿›ä¸å» Decoding Poolï¼Œ
        # æˆ–è€… Decoding Pool å¹¶å‘è¿‡é«˜å¯¼è‡´æ˜¾å­˜å¸¦å®½äº‰æŠ¢ã€‚
        # æˆ‘ä»¬å¯ä»¥ç”¨ (Decodingé˜Ÿåˆ— / Decodingæ¶ˆåŒ–é€Ÿåº¦) æ¥è¡¡é‡â€œç­‰å¾…è¿›å…¥ Decoding çš„å»¶è¿Ÿâ€ã€‚
        # å¦‚æœ Decoding é˜Ÿåˆ—åœ¨å †ç§¯ï¼Œè¯´æ˜ TBT é£é™©æå¤§ (å› ä¸ºå‰é¢çš„è¯·æ±‚å¡ä½äº†)ã€‚
        est_decoding_wait = q_decoding / throughput_d

        # å½’ä¸€åŒ–ä¸º Ratio (ç›¸å¯¹äº SLO é˜ˆå€¼)
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸»è¦ç”¨ est_ttft æ¥æƒ©ç½š Prefill ä¸è¶³
        # ç”¨ est_decoding_wait æ¥æƒ©ç½š Decoding ä¸è¶³
        ratio_ttft = est_ttft / self.TARGET_TTFT

        # å¯¹äº TBTï¼Œé™¤äº†æ’é˜Ÿï¼Œè¿˜è¦çœ‹å½“å‰çš„æ˜¾å­˜å¸¦å®½å‹åŠ›
        # å¦‚æœæ²¡æœ‰æ’é˜Ÿï¼Œä½† Token Generation Rate å¾ˆé«˜ï¼ŒTBT ä¹Ÿä¼šå·®ã€‚
        # è¿™é‡Œç”¨ä¸€ç§æ··åˆæŒ‡æ ‡ï¼š
        # å¦‚æœæœ‰æ’é˜Ÿï¼Œæƒ©ç½šæ’é˜Ÿï¼›å¦‚æœæ²¡æœ‰æ’é˜Ÿï¼Œæƒ©ç½šæ½œåœ¨çš„å¸¦å®½æ‹¥å µï¼ˆå¯é€‰ï¼Œç®€å•èµ·è§å…ˆåªçœ‹æ’é˜Ÿï¼‰
        ratio_tbt = est_decoding_wait / (self.TARGET_TBT * 10)  # å®¹å¿åº¦ç¨å¾®æ”¾å®½ï¼Œå› ä¸ºæ’é˜Ÿåªæ˜¯ TBT çš„ä¸€éƒ¨åˆ†å› ç´ 

        max_ratio = max(ratio_ttft, ratio_tbt)

        # -------------------------------------------------------------
        # 3. è®¡ç®—å¥–åŠ± (é€»è¾‘ä¸ä¹‹å‰ç›¸åŒï¼Œä½†è¾“å…¥å˜äº†)
        # -------------------------------------------------------------

        # è®¡ç®—æˆæœ¬åˆ†æ•°
        n_p, n_t = raw_stats[4:6]
        cost_score = (n_p + n_t ) / self.max_instances

        reward = 0.0

        if max_ratio > 1.0:
            # === ğŸ”´ å±é™©åŒº ===
            # é˜Ÿåˆ—å †ç§¯å¯¼è‡´é¢„ä¼°å»¶è¿Ÿè¶…æ ‡ï¼Œç«‹åˆ»é‡ç½šï¼
            # è¿™æ · Agent åœ¨é˜Ÿåˆ—åˆšå¼€å§‹å †ç§¯ï¼ˆtæ—¶åˆ»ï¼‰å°±ä¼šæ”¶åˆ°è´Ÿåé¦ˆï¼Œä¸ç”¨ç­‰è¯·æ±‚è·‘å®Œã€‚
            reward = -self.BASE_SLO_PENALTY * ((max_ratio - 1.0) ** 2) - 2.0

        elif max_ratio > 0.8:
            # === ğŸŸ¡ ç¼“å†²åŒº ===
            reward = (1.0 - cost_score) + 0.5

        else:
            # === ğŸŸ¢ å®‰å…¨åŒº ===
            reward = 1.0 - cost_score

        # -------------------------------------------------------------
        # 4. ç¨³å®šæ€§æƒ©ç½š
        # -------------------------------------------------------------
        # (ä¿æŒåŸæœ‰çš„è¿Ÿæ»æƒ©ç½šé€»è¾‘)
        # ...
        info = {
            'step':step,
            'reward':reward,
            'ratio_ttft': ratio_ttft,
            'ratio_tbt': ratio_tbt,
            'max_ratio': max_ratio,
            'cost_score': cost_score,
        }
        return reward,info

    def calculate_reward_(self, cluster, applications, interval_stats, instance_num, action_executed=True):
        """
        è®¡ç®—å•æ­¥å¥–åŠ±
        :param cluster: Cluster å¯¹è±¡
        :param applications: App å¯¹è±¡
        :param interval_stats: åŒ…å« TTFTå’ŒTBTçš„P50ã€P90ã€P99å€¼çš„åˆ—è¡¨
                               æ ¼å¼ï¼š[[ttft_p50, ttft_p90, ttft_p99], [tbt_p50, tbt_p90, tbt_p99], [ttft_vio, tbt_vio]]
        :param instance_num: å®ä¾‹æ•°é‡å’Œåˆ©ç”¨ç‡
        :param action_executed: æ˜¯å¦æ‰§è¡Œäº†æ‰©ç¼©å®¹åŠ¨ä½œ
        :return: (total_reward, info_dict)
        """

        # --- A. è¿è¥æˆæœ¬é¡¹ (OpEx) ---
        # ç›®æ ‡ï¼šæœ€å°åŒ–ç§Ÿé‡‘
        n_p, n_t = instance_num[0], instance_num[1]

        # è®¡ç®—åŠ æƒæˆæœ¬ (Normalized by max budget)
        current_cost = (n_p * self.price_p + n_t * self.price_t)
        max_possible_cost = self.max_instances * 1.0

        cost_penalty = -current_cost

        # --- B. SLO å¥–åŠ±é¡¹ (Performance) ---
        # ä½¿ç”¨ P50ã€P90ã€P99 çš„ TTFT å’Œ TBT è®¡ç®— SLO åˆè§„æ€§
        # SLO é˜ˆå€¼ï¼š
        # TTFT: P50=2, P90=3, P99=6
        # TBT: P50=1.25, P90=1.5, P99=5
        
        ttft_values = interval_stats[0]  # [p50, p90, p99]
        tbt_values = interval_stats[1]   # [p50, p90, p99]
        
        # TTFT SLO é˜ˆå€¼
        ttft_slo_thresholds = [2.0, 3.0, 6.0]  # P50, P90, P99
        # TBT SLO é˜ˆå€¼
        tbt_slo_thresholds = [1.25, 1.5, 5.0]  # P50, P90, P99
        
        # è®¡ç®—æ¯ä¸ªåˆ†ä½æ•°çš„åˆè§„ç‡ï¼ˆå€¼è¶Šå°äºé˜ˆå€¼è¶Šå¥½ï¼‰
        ttft_compliance_scores = []
        tbt_compliance_scores = []
        
        # å¥–åŠ±é€»è¾‘ï¼šSLO ç¬¦åˆ (â‰¤ é˜ˆå€¼) ç»™å°å¥–åŠ±ï¼Œä¸ç¬¦åˆ (> é˜ˆå€¼) ç»™æƒ©ç½š
        ttft_compliance_scores = []
        tbt_compliance_scores = []
        for i in range(3):
            # TTFT
            if ttft_values[i] <= ttft_slo_thresholds[i]:
                ttft_score = 10  # ç¬¦åˆ SLO ç»™å°å¥–åŠ±
            else:
                ttft_score = -1 * (ttft_values[i] - ttft_slo_thresholds[i]) *10 # è¶…å‡º SLO ç»™æƒ©ç½šï¼ŒæŒ‰è¶…å‡ºæ¯”ä¾‹çº¿æ€§
            ttft_compliance_scores.append(ttft_score)
            # TBT
            if tbt_values[i] <= tbt_slo_thresholds[i]:
                tbt_score = 10
            else:
                tbt_score = -1 * (tbt_values[i] - tbt_slo_thresholds[i]) *10
            tbt_compliance_scores.append(tbt_score)
        # åŠ æƒå¹³å‡
        weights = [0.2, 0.3, 0.5]
        ttft_weighted_score = sum(w * s for w, s in zip(weights, ttft_compliance_scores))
        tbt_weighted_score = sum(w * s for w, s in zip(weights, tbt_compliance_scores))
        
        # ç»¼åˆ SLO å¥–åŠ±ï¼ˆTTFT å’Œ TBT å„å ä¸€åŠï¼‰
        slo_reward = 0.5 * ttft_weighted_score + 0.5 * tbt_weighted_score
        

        # --- C. åˆ‡æ¢æˆæœ¬é¡¹ & ç¨³å®šæ€§å¥–åŠ± (Stability) ---
        # ç›®æ ‡ï¼šæŠ‘åˆ¶æœºå™¨æ•°é‡å‰§çƒˆæŠ–åŠ¨ï¼Œå¥–åŠ±ç¨³å®šçŠ¶æ€
        stability_bonus = 0.0
        
        if self.is_first_step:
            self.is_first_step = False
            switch_penalty = 0.0
        else:
            # å¦‚æœæ²¡æœ‰æ‰§è¡Œæ‰©ç¼©å®¹åŠ¨ä½œï¼ˆdelta_total == 0 æˆ– action_executed == Falseï¼‰
            if not action_executed:
                # ç»™äºˆç¨³å®šæ€§å¥–åŠ±
                stability_bonus = 5  # é¼“åŠ±ä¿æŒç¨³å®š
                switch_penalty = 0.0
            else:
                switch_penalty = -5

    
        delta_p = abs(n_p - self.last_instances['p'])
        delta_t = abs(n_t - self.last_instances['t'])
        delta_total = delta_p + delta_t
            
        # æ›´æ–°å†å²
        self.last_instances = {'p': n_p, 't': n_t}

        # --- D. åˆ©ç”¨ç‡å¡‘å½¢ (Reward Shaping - Optional) ---
        # ç›®æ ‡ï¼šå¼•å¯¼ Agent å°†åˆ©ç”¨ç‡ç»´æŒåœ¨ "Sweet Spot" (ä¾‹å¦‚ 60% - 80%)
        # é¿å… 0% (æµªè´¹) ä¹Ÿä¸è¦ 100% (å®¹æ˜“æ’é˜Ÿ)
        util_p,util_d = instance_num[2],instance_num[3]

        def utilization_bonus(u):
            # ä¸€ä¸ªå€’ U å‹å‡½æ•°ï¼Œåœ¨ 0.7 å¤„è¾¾åˆ°å³°å€¼ 1.0
            # æ”¹ä¸ºæŒ‡æ•°å½¢å¼ä»¥å¢å¼ºæ•æ„Ÿåº¦
            return np.exp(1.0 - abs(u - 0.7)) / np.e

        util_reward = 0.3 * utilization_bonus(util_p) + 0.3 * utilization_bonus(util_d)

        # --- E. æ€»å¥–åŠ±èšåˆ ---
        # æ³¨æ„ï¼šCost å’Œ Switch æ˜¯è´Ÿå€¼ï¼ŒSLOã€Util å’Œ Stability æ˜¯æ­£å€¼
        # æ ‡å‡†åŒ–å„ç»„ä»¶å€¼åˆ°ç›¸ä¼¼èŒƒå›´ï¼Œä¿æŒç¬¦å·ä¸å˜
        total_reward = (
                self.w_cost * cost_penalty +
                self.w_slo * slo_reward +
                self.w_switch * (switch_penalty + stability_bonus)
                # self.w_util * util_reward
        )

        # è¿”å›è¯¦ç»†ä¿¡æ¯ç”¨äº Debug (è¿™å¯¹ RL è°ƒå‚è‡³å…³é‡è¦ï¼)
        info = {
            "reward_total": total_reward,
            "raw_cost": current_cost,
            "pen_cost": self.w_cost * cost_penalty,
            "rew_slo": self.w_slo * slo_reward,
            "pen_switch": self.w_switch * switch_penalty,
            "rew_stability": self.w_switch * stability_bonus,
            "rew_util": self.w_util * util_reward,
            "ttft_weighted": ttft_weighted_score,
            "tbt_weighted": tbt_weighted_score,
            "ttft_p50": ttft_values[0] if len(ttft_values) > 0 else 0,
            "ttft_p90": ttft_values[1] if len(ttft_values) > 1 else 0,
            "ttft_p99": ttft_values[2] if len(ttft_values) > 2 else 0,
            "tbt_p50": tbt_values[0] if len(tbt_values) > 0 else 0,
            "tbt_p90": tbt_values[1] if len(tbt_values) > 1 else 0,
            "tbt_p99": tbt_values[2] if len(tbt_values) > 2 else 0,
            "delta_total": delta_total,
            "action_executed": action_executed,
            "util_avg": (util_p + util_d) / 2
        }

        return total_reward, info

    def reset(self):
        """é‡ç½®å†…éƒ¨çŠ¶æ€ (æ¯ä¸ª Episode å¼€å§‹æ—¶è°ƒç”¨)"""
        self.last_instances = {'p': 0, 't': 0}

class RewardRecorder:


    def __init__(self, filename="reward.csv", clear_file=True):
        self.filename = filename
        self.fieldnames = None  # å°†åœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨record_rewardæ—¶åˆå§‹åŒ–
        self._initialize_csv(clear_file)

    def _initialize_csv(self, clear_file=True):
        """Initialize the CSV file with headers"""
        # å¦‚æœéœ€è¦æ¸…ç©ºæ–‡ä»¶æˆ–è€…æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™é‡æ–°åˆ›å»ºæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´
        if clear_file or not os.path.exists(self.filename):
            with open(self.filename, 'w', newline='') as csvfile:
                pass  # åªåˆ›å»ºç©ºæ–‡ä»¶

    def record_reward(self, step, info_dict):
        """
        Record reward components to CSV file

        :param step: Current decision step
        :param info_dict: Dictionary containing reward components from RLRewardCalculator
        """

        # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼Œåˆå§‹åŒ–fieldnameså¹¶å†™å…¥è¡¨å¤´
        if self.fieldnames is None:
            self.fieldnames = list(info_dict.keys())
            with open(self.filename, 'w', newline='') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=self.fieldnames)
                writer.writeheader()

        # ä½¿ç”¨è¿½åŠ æ¨¡å¼å†™å…¥æ•°æ®
        with open(self.filename, 'a', newline='') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=self.fieldnames)
            writer.writerow(info_dict)